Classification & Regression Trees

9.1 Introduction

Trees
Decision trees can be used both for classification and regression, but mostly classification. 


Entropy & information gain --> Ace the Data Science Interview(pg 97 - pg 100)
if there is a lot of impurity in the data, then there will be less nformation gain
if it is pure data meaning less impurity, more info gain
- our root node in a decision tree is the one that has the most information gain
ðŸ”¸ High entropy â†’ more randomness, variety, or unpredictability
ðŸ”¸ Low entropy â†’ repetitive, predictable, or uniform

Ginni Index
<img width="687" height="462" alt="image" src="https://github.com/user-attachments/assets/13019197-1067-4902-b1ad-090d83f031b2" />

Add 2 imgs or notes:
- numerbs for entropy, how do I know I should use a specific variable or not
- numbers for gini index, how do I know I should use a specific variable or not


Cart Vs Random Forest
Think of it like this ðŸ‘‡

ðŸ§  CART = one expertâ€™s opinion
ðŸŒ² Random Forest = a panel of experts â€” each looks at slightly different data and features, then they vote or average their answers.



Bagging




All of these are strings you can pass to scoring= in GridSearchCV:

Scoring string	What it means	Notes
'r2'	RÂ² (coefficient of determination)	Higher is better, range ~(-âˆž, 1]
'neg_mean_squared_error'	
âˆ’
MSE
âˆ’MSE	What youâ€™re using now
'neg_root_mean_squared_error'	
âˆ’
RMSE
âˆ’RMSE	Easier to read than MSE (same units as target)
'neg_mean_absolute_error'	
âˆ’
MAE
âˆ’MAE	Robust to outliers
'neg_mean_absolute_percentage_error'	
âˆ’
MAPE
âˆ’MAPE	Error as a percentage (careful with values near 0)
'neg_median_absolute_error'



<img width="866" height="492" alt="image" src="https://github.com/user-attachments/assets/5c5e0f27-e485-44b7-854b-461bea971f36" />



                    ðŸŒ³ Decision Tree Algorithms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”œâ”€â”€ Single Tree Algorithms
   â”‚     â”œâ”€â”€ ID3 â†’ uses entropy
   â”‚     â”œâ”€â”€ C4.5 â†’ uses gain ratio
   â”‚     â””â”€â”€ CART â†’ uses gini / MSE
   â”‚
   â”œâ”€â”€ Ensemble (Multiple Trees)
   â”‚     â”œâ”€â”€ Bagging â†’ parallel independent trees
   â”‚     â”‚      â””â”€â”€ Random Forest = Bagging + feature randomness
   â”‚     â”‚
   â”‚     â””â”€â”€ Boosting â†’ sequential dependent trees
   â”‚            â”œâ”€â”€ AdaBoost â†’ reweight errors
   â”‚            â”œâ”€â”€ Gradient Boosting â†’ fit residuals
   â”‚            â”œâ”€â”€ XGBoost â†’ optimized gradient boosting
   â”‚            â”œâ”€â”€ LightGBM â†’ gradient boosting with leaf growth
   â”‚            â””â”€â”€ CatBoost â†’ boosting for categorical data


ðŸ§© Quick Analogy
Category	Analogy
CART / ID3 / C4.5	One decision tree (one student taking the exam)
Bagging / Random Forest	Many students take the exam separately â†’ average their answers
Boosting (AdaBoost, XGBoost, etc.)	Students take the exam sequentially, each learning from the previous studentâ€™s mistakes


